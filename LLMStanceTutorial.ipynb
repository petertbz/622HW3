{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for stance detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Large Language Models to perform stance detection. The task is to classify the stance of a given text towards a given target. And as I mentioned, the LLMs can be used to perform this task with Few-Shot or even Zero-Shot learning. I will provide a Zero-Shot learning example in this notebook.\n",
    "\n",
    "I will demonstrate how to use the LLMs for stance detection using the `transformers` library. We will use the `mistralai/Mixtral-8x7B-Instruct-v0.1` model for this task. The `transformers` library provides a simple API to use the LLMs for various NLP tasks.\n",
    "\n",
    "We will use the `AutoModelForCausalLM` class to load the model and the `AutoTokenizer` class to load the tokenizer. Causal language models are the models that can generate the next token given the previous tokens and tokenizer is used to convert the text into tokens that can be fed into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "stance = pd.read_csv(\"../../data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load Huggingface Transformers library\n",
    "# https://huggingface.co/transformers/\n",
    "# clear jupyter notebook output\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")  # Load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=device)  # Load model\n",
    "clear_output()\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start format the prompt template, we need to understand what the inputs should look like and why we need to format the prompt template like this.\n",
    "\n",
    "For training a Large Language Model, we need to provide the model with a prompt that contains the input text and the target text. And the model should be able to distinguish between human input and desired output. Therefore, we will roughly see two types of prompt templates: one will only distinguish between the human input and model output and the other will also provide the instruction. The only difference between the two is that the former will treat the instruction as a part of the input text and the latter will treat the instruction as a separate entity.\n",
    "\n",
    "Based on the [model specification](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) provided by Mistral AI, they used the first type of prompt template. Therefore, we will also follow the same format for the prompt template.\n",
    "\n",
    "The prompt template should look like this:\n",
    "\n",
    "```\n",
    "<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n",
    "```\n",
    "\n",
    "Where `<s>` and `</s>` are the special tokens that are used to indicate the start and end of the sequence. `[INST]` and `[/INST]` are the special tokens that are used to indicate the start and end of the instruction. `<input_text>` is the input text that we want to classify.\n",
    "\n",
    "Luckily, for inference only task (i.e., zero-shot or few-shot learning), we don't need to provide the model with the target text. We only need to provide the model with the input text. And we only need to do single round of inference. For the spacial tokens, the model will automatically add them to the input text.\n",
    "\n",
    "To reuse the prompt template for different inputs, we will create a `f` string that will take the input text and the target text as input and return the formatted prompt template. If you are not familiar with the `f` string, you can think of it as a string that can take variables as input and return the formatted string. More details can be found [here](https://realpython.com/python-f-strings/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example to understand how the prompt template will look like for the stance detection task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Economy is better now than before\"\n",
    "comment = stance.loc[0, \"comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Economy is better now than before\n",
      "Post/Comment: Most you\u0019ve ever made nominally could still be less than you made 5 years ago in real terms if your* raises since then are not in the 25% range. Inflation Jan 2019 to Jan 2024: 22.53%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Topic: {topic}\\nPost/Comment: {comment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for i in range(838):\n",
    "    prompts.append(f\"\"\"\n",
    "                    Instruction: You have assumed the role of a human annotator. In this task, you will be presented with a reddit post/comment, delimited by triple backticks, concerning whether the {topic}. Please make the following assessment:\n",
    "                    (1) Determine whether the comment/post discusses the topic of whether the {topic}. If so, please indicate whether the Reddit user who posted the tweet favors, opposes, or is neutral about whether the {topic}.\n",
    "\n",
    "                    Your response should be formatted as follows and include nothing else: \"Stance: [F, O, N]\"\n",
    "\n",
    "                    Here F stands for Favor, O stands for Oppose and N stands for Neutral.\n",
    "\n",
    "                    Post/Comment: ```{stance.loc[i, 'comment']}```\n",
    "                   \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "                    Instruction: You have assumed the role of a human annotator. In this task, you will be presented with a reddit post/comment, delimited by triple backticks, concerning whether the Economy is better now than before. Please make the following assessment:\n",
      "                    (1) Determine whether the comment/post discusses the topic of whether the Economy is better now than before. If so, please indicate whether the Reddit user who posted the tweet favors, opposes, or is neutral about whether the Economy is better now than before.\n",
      "\n",
      "                    Your response should be formatted as follows and include nothing else: \"Stance: [F, O, N]\"\n",
      "\n",
      "                    Here F stands for Favor, O stands for Oppose and N stands for Neutral.\n",
      "\n",
      "                    Post/Comment: ```\"It is estimated there is more than $6 trillion in money markets currently and most of those accounts are earning close to 5%,\" Detrick said.\u001d\n",
      "\n",
      "\n",
      "Also plenty of HYSA out there currently:\n",
      "\n",
      "https://www.nerdwallet.com/best/banking/high-yield-online-savings-accounts```\n",
      "                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prompt: {prompts[836]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the input text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the raw input text, we will need to transform it into the format that the model can understand. We will use the `AutoTokenizer` class to convert the input text into tokens that can be fed into the model. The `AutoTokenizer` class will automatically select the appropriate tokenizer for the model.\n",
    "\n",
    "For more information on understand how tokenizer works and how to use the `AutoTokenizer` class, you can refer to the [official documentation](https://huggingface.co/transformers/model_doc/auto.html#autotokenizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[    1, 28705,    13,   359,  2287,  3133,  3112, 28747,   995,   506,\n",
      "         11012,   272,  3905,   302,   264,  2930,   396,  1478,  1028, 28723,\n",
      "           560,   456,  3638, 28725,   368,   622,   347,  7567,   395,   264,\n",
      "         22003,  1704, 28748,  9318, 28725,   882,   321,  1345,   486, 22212,\n",
      "           852, 28707,  5446, 28725, 15942,  3161,   272, 11003, 28724,   349,\n",
      "          1873,  1055,   821,  1159, 28723,  5919,  1038,   272,  2296, 15081,\n",
      "         28747,    13,   359,  2287,   325, 28740, 28731,  5158, 21824,  3161,\n",
      "           272,  4517, 28748,  3192,  3342,   274,   272,  9067,   302,  3161,\n",
      "           272, 11003, 28724,   349,  1873,  1055,   821,  1159, 28723,  1047,\n",
      "           579, 28725,  4665, 11634,  3161,   272, 22233,  2188,   693, 10198,\n",
      "           272,  9394,   299,  7556,   734, 28725,  5793,   274, 28725,   442,\n",
      "           349, 14214,   684,  3161,   272, 11003, 28724,   349,  1873,  1055,\n",
      "           821,  1159, 28723,    13,    13,   359,  2287,  3604,  2899,  1023,\n",
      "           347,  1221, 11985,   390,  6104,   304,  3024,  2511,  1112, 28747,\n",
      "           345,   718,   617, 28747,   733, 28765, 28725,   451, 28725,   418,\n",
      "         28793, 28739,    13,    13,   359,  2287,  4003,   401, 10969,   354,\n",
      "           401,  3115, 28725,   451, 10969,   354, 22396,   645,   304,   418,\n",
      "         10969,   354,  3147,   329,  1650, 28723,    13,    13,   359,  2287,\n",
      "          5112, 28748, 13617, 28747,  8789,   657,  1134,   352,  2368, 28742,\n",
      "         28707,  7350,   368, 28725,   486,  3837, 28723, 28705,   661,  7033,\n",
      "          1074,  5206,   404,   304,   905,   395,  7877, 28723, 28705,   415,\n",
      "           865,  1970,   369,  7033,  1074, 13207,  7433,   349, 20613, 11862,\n",
      "          9556,   821, 24110, 28723, 28705, 22377,  2492,  1864,   264,   937,\n",
      "          1893,  6881,  7033,  1074,   272,  1474,   302, 22271,   693,   829,\n",
      "         15270,   368, 28723, 13940, 28832,    13,   359,  2287]],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "inputs_list = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    inputs_list.append(inputs)\n",
    "\n",
    "print(f\"Tokens: {inputs_list[4]['input_ids']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the tokenized prompt can always be decoded back to the original prompt\n",
    "# Decode the tokenized prompt\n",
    "# decoded_prompt = tokenizer.decode(inputs['input_ids'][0])\n",
    "# print(decoded_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed the input text (tokens) into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_list = []\n",
    "stance['LLM Stance'] = ''\n",
    "\n",
    "for i in range(837):\n",
    "    outputs = model.generate(**inputs_list[i], max_new_tokens=20)  # Generate the model output\n",
    "    # Decode the generated output\n",
    "    generated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    stance.loc[i, 'LLM Stance'] = generated_output[len(prompt):]\n",
    "    print(i)\n",
    "    \n",
    "\n",
    "stance.to_csv(\"../../data/updated_cleaned_data.csv\", index=False)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several parameters that we can use to control the output of the model. We used the `max_length` parameter to control the maximum length of the output. We alsod use the `return_tensors` parameter to control the output format. We set it to `pt` to get the output in PyTorch tensors format.\n",
    "\n",
    "Besides these parameters, we can also use the `temperature` parameter to control the randomness of the output. We can use the `top_k` and `top_p` parameters to control the diversity of the output. We can also use the `num_return_sequences` parameter to control the number of output sequences.\n",
    "\n",
    "There is a great explanation of temperature parameter (which is also the parameter you will use for OpenAI's Models) in the [blog](https://lukesalamone.github.io/posts/what-is-temperature/)\n",
    "\n",
    "Feel free to play around with these parameters to see how they affect the output of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing performance measures of the LLM\n",
    "\n",
    "df = pd.read_csv('../../data/updated_cleaned_data.csv')  \n",
    "\n",
    "human_labels = df['label'].tolist()\n",
    "llm_labels = df['LLM Stance'].tolist()\n",
    "\n",
    "# Calculating metrics\n",
    "accuracy = accuracy_score(human_labels, llm_labels)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Calculating precision, recall, and F1-score for each class\n",
    "precision, recall, f1, support = precision_recall_fscore_support(human_labels, llm_labels, labels=[\"F\", \"O\", \"N\"])\n",
    "print(\"Precision per class:\")\n",
    "for label, prec in zip([\"F\", \"O\", \"N\"], precision):\n",
    "    print(f'{label}: {prec:.2f}')\n",
    "\n",
    "print(\"Recall per class:\")\n",
    "for label, rec in zip([\"F\", \"O\", \"N\"], recall):\n",
    "    print(f'{label}: {rec:.2f}')\n",
    "\n",
    "print(f'Precision (Weighted): {precision.mean():.2f}')\n",
    "print(f'Recall (Weighted): {recall.mean():.2f}')\n",
    "print(f'F1-Score (Weighted): {f1.mean():.2f}')\n",
    "\n",
    "# Calculating specificity for each class\n",
    "cm = confusion_matrix(human_labels, llm_labels, labels=[\"F\", \"O\", \"N\"])\n",
    "specificity_scores = []\n",
    "for i in range(len(cm)):\n",
    "    true_negatives = sum(np.delete(np.delete(cm, i, 0), i, 1).flatten())\n",
    "    false_positives = sum(np.delete(cm[i], i))\n",
    "    specificity = true_negatives / (true_negatives + false_positives)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "print(f'Specificity per class (F, O, N): {specificity_scores}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
